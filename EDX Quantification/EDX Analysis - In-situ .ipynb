{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDX Analysis for In-situ Heating Experiments from the Thermo Fisher Velox Software"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a personal workbook used to process my own data but has been made public for the use of others. If you find any errors or need further help using this workbook please contact by email k8macarthur@gmail.com."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors:\n",
    "\n",
    "Katherine E. MacArthur: Originally written upon my leaving the Ernst Ruska Centre August 2021 for knowledge transfer.\n",
    "\n",
    "## Requirements\n",
    "\n",
    "This workbook was written with Hyperspy-base v1.6.3, numpy v1.20.2, matplotlib v3.4.2. Earlier versions than this may be possible but have not been tested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing Libraries and Checking Parameters\n",
    "\n",
    "We begin be importing the necessary libraries. \n",
    "\n",
    "It's also import to use this section to check the parameter selection (e.g. number of frames for EDX and number of components for PCA) before carrying out the full automated extraction below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "import matplotlib\n",
    "import hyperspy.api as hs\n",
    "import scipy.misc\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading_SI_image_stack=True means the whole ADF image stack is imported.\n",
    "\n",
    "If a file_name is not provided then a window will pop up to allow selection of the file.\n",
    "\n",
    "\n",
    "Including both a first_frame and last_frame allows to only select a portion of the EDX dataset in the time dimension. This should be used to test that enough signal can be reached with the desired time resolution (or window width in the time dimension.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_frame = 0\n",
    "last_frame = 150\n",
    "s = hs.load(first_frame=first_frame, last_frame =last_frame, load_SI_image_stack=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also important to check the order of the imported list. \n",
    "\n",
    "At the time of writing the exact position of the HAADF image (or image stack) changes for each data set. Even during when acquiring multiple datasets back-to-back with the same element settings. However the EDSSTEMSpectrum (integrating all detectors) is always at the end of the list and each of the ESDTEMSpectrum (individual) are at the start of the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<EDSTEMSpectrum, title: EDS, dimensions: (|4096)>,\n",
       " <EDSTEMSpectrum, title: EDS, dimensions: (|4096)>,\n",
       " <EDSTEMSpectrum, title: EDS, dimensions: (|4096)>,\n",
       " <EDSTEMSpectrum, title: EDS, dimensions: (|4096)>,\n",
       " <Signal2D, title: HAADF, dimensions: (494|532, 928)>,\n",
       " <Signal2D, title: Pt, dimensions: (|532, 928)>,\n",
       " <Signal2D, title: Ni, dimensions: (|532, 928)>,\n",
       " <EDSTEMSpectrum, title: EDS, dimensions: (532, 928|4096)>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time_step =  121.31483092617837 min\n"
     ]
    }
   ],
   "source": [
    "#I like to store the ADF_location here for later use\n",
    "ADF_location = 5\n",
    "#Here we print the time resolution step so you can see the duration of the EDX map.\n",
    "print('Time_step = ', s[ADF_location].axes_manager[0].scale*1000*(last_frame-first_frame)/60, 'min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line will save the ADF image stack into a tif stack.\n",
    "s[-2].save('1535_ADF_Stack.tif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the EDX data analsysis we being by plotting the raw intgrated data cube. \n",
    "Although at this stage it can be that we don't see a lot of signal in just the raw map.\n",
    "\n",
    "Typically I crop the first 0.1eV from the start of the signal as I find there is a hardware peak there which messes up the intensity plotting of the map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = s[-1].isig[0.1:]\n",
    "#Not this is now over writing our original s.\n",
    "#but having saved the ADF stack we don't often have use for \n",
    "#the rest of the raw data cube so this helps to save memory.\n",
    "s.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next it's important to adjust our rebin parameter. In my experience no high resolution EDX map can easily be processed without some binning. I am using the rebin function in conjunction with the plotting function to see the results. Once I'm happy with the number of bins I want to provide I can then go ahead and overwrite the signal again with a newly rebined signal.\n",
    "\n",
    "A quick marker for the correct amount of binning is whether or not you can start to see a basic peak shape per pixel for the elements which will be quantified. The PCA analysis will later also help to show if the data has been sufficiently rebinned.\n",
    "\n",
    "Obviously, if the spatial binning level needed is to high here you can go back and select a larger time dimension during import at the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.rebin(scale=(15,15,2)).plot() \n",
    "#Here we are rebinning by 15 in both spatial dimensions and by 2 in the energy dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = s.rebin(scale=(15,15,2))\n",
    "#This step saves and overwrites our spectrum data cube."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, because we're working on the low end of the signal-to-noise level I would always perfom seom PCA denoising. Again this should also be checked before we set up the automated analysis.\n",
    "\n",
    "We start by performing a basic PCA decomposition and plotting the variance ratio.\n",
    "\n",
    "Here we're looking for confirmation that the variance plot produces a clean elbow shaped graph that shows only a small number of clearly defined components (loadings) can be seen above the noise level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposition info:\n",
      "  normalize_poissonian_noise=True\n",
      "  algorithm=SVD\n",
      "  output_dimension=None\n",
      "  centre=None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'EDS\\nPCA Scree Plot'}, xlabel='Principal component index', ylabel='Proportion of variance'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.change_dtype('float64')\n",
    "s.decomposition()\n",
    "s.plot_explained_variance_ratio()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the PCA decomposition we now know the estimated number of components that could then be used for the denoising. For EDX analysis I prefer to use NMF (non-negative factorization) rather than PCA because this forces the restriction that all components must be strictly real and positive.\n",
    "\n",
    "Once we have decomposed the data cube. We then reassemble the data cube with only the components that we consider above the noise. This technique is referred to as de-noising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macark/hyperspy-bundle/lib/python3.8/site-packages/hyperspy/learn/mva.py:271: VisibleDeprecationWarning: The algorithm name `nmf` has been deprecated and will be removed in HyperSpy 2.0. Please use `NMF` instead.\n",
      "  warnings.warn(\n",
      "/Users/macark/hyperspy-bundle/lib/python3.8/site-packages/sklearn/decomposition/_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn((\"The 'init' value, when 'init=None' and \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposition info:\n",
      "  normalize_poissonian_noise=True\n",
      "  algorithm=NMF\n",
      "  output_dimension=2\n",
      "  centre=None\n",
      "scikit-learn estimator:\n",
      "NMF(n_components=2)\n"
     ]
    }
   ],
   "source": [
    "s.decomposition(algorithm='NMF', output_dimension=2)\n",
    "#s.plot_explained_variance_ratio()\n",
    "s.plot_decomposition_loadings(2)\n",
    "sc = s.get_decomposition_model(2)\n",
    "sc.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're not sure exactly what number of components to use for the reconstructed EDX data cube, you can look at the difference between 'n' and 'n+1' components check there is only residual noise difference and nothing structural."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macark/Documents/Git/hyperspy/hyperspy/drawing/utils.py:133: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig = plt.figure(**kwargs)\n",
      "/Users/macark/Documents/Git/hyperspy/hyperspy/drawing/utils.py:133: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  fig = plt.figure(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "s.decomposition(algorithm='nmf', output_dimension=3)\n",
    "sc2 = s.get_decomposition_model(3)\n",
    "(sc2-sc).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we check the metadata and set the require elements for extraction of maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sc.add_elements(['C'])\n",
    "sc.set_lines(['Ni_Ka', 'Pt_Lb1', 'Pt_Lb2'])\n",
    "sc.plot(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we get the X-ray lines and extract them. Here we're not yet applying a background subtraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<BaseSignal, title: X-ray line intensity of EDS model from decomposition with 2 components: Ni_Ka at 7.48 keV, dimensions: (35, 61|)>,\n",
       " <BaseSignal, title: X-ray line intensity of EDS model from decomposition with 2 components: Pt_Lb1 at 11.07 keV, dimensions: (35, 61|)>,\n",
       " <BaseSignal, title: X-ray line intensity of EDS model from decomposition with 2 components: Pt_Lb2 at 11.25 keV, dimensions: (35, 61|)>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = sc.get_lines_intensity()\n",
    "lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next lines of code can be used to plot each of the X-ray maps in your list of maps. The save function also saves these for later if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in lines:\n",
    "    line.plot()\n",
    "    Image.fromarray(line.data).save('1534_'+line.metadata.Sample.xray_lines[0]+'.tif') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Running the main EDX In-situ Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having checked the parameters we can now run a full in-situ analysis for an EDX map.\n",
    "\n",
    "Beginning with setting the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_range = 250 #How many ADF frames do you want include into each EDX map.\n",
    "overlap = 100 #In order to maintain time resolution you can add an additional overlap\n",
    "no_of_frames_1 = 442 #This can be determined from the data but is easier if set here after checking once.\n",
    "file_name_1 = '1406 EDS-HAADF 2.0 Mx.emd' #The file name is needed as we'll be loading many times and \n",
    "                                           #don't want to have to use the interactive window each time.\n",
    "\n",
    "file_location = '/Users/macark/Desktop/TUB-EDX-PtNiX/In-Situ_Heating_Feb20/300C/'\n",
    "save_folder = '1406'\n",
    "adf_location = 5\n",
    "bin_scale = (8,8,2)\n",
    "no_of_components = 3 #for nmf decomposition\n",
    "\n",
    "#Parameters for quantification.\n",
    "beam_current = 0.06466 #in nA\n",
    "elements = ['Pt', 'Ni']\n",
    "lines = ['Ni_Ka', 'Ni_Kb', 'Pt_Lb1', 'Pt_Lb2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Signal2D, title: HAADF, dimensions: (250|532, 928)>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s[adf_location].inav[191:441]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If using the quantification part of the code then you need to import or select calibrations the follow steps do this for EDX cross_sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration = hs.load('ChemiSTEMCal_80kV_15.04.20.hspy').metadata.calibration\n",
    "cs = (((calibration['Ni_Ka']['cross_section'] + calibration['Ni_Kb']['cross_section']),\n",
    "       (calibration['Ni_Ka']['cross_section'] + calibration['Ni_Kb']['cross_section']),\n",
    "       (calibration['Pt_La']['cross_section'] + calibration['Pt_Lb1']['cross_section']),\n",
    "       (calibration['Pt_La']['cross_section'] + calibration['Pt_Lb1']['cross_section'])))\n",
    "iw = calibration['Ni_Ka']['Integration windows'] + calibration['Pt_La']['Integration windows']\n",
    "bw = np.append(calibration['Ni_Ka']['Background windows'], \n",
    "               calibration['Pt_La']['Background windows'],\n",
    "                  axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the next section of code you will need to run the two functions which are stored at the bottom of the work book. It's worth noting here that the open_and_save function will run and save count maps for any number of elements defined in the list above. The quantify and save function was only written for the Pt Ni binary investigations and will need to modified to include other elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Signal2D, title: HAADF, dimensions: (442|532, 928)>\n",
      "Decomposition info:\n",
      "  normalize_poissonian_noise=True\n",
      "  algorithm=SVD\n",
      "  output_dimension=None\n",
      "  centre=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macark/hyperspy-bundle/lib/python3.8/site-packages/sklearn/decomposition/_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn((\"The 'init' value, when 'init=None' and \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposition info:\n",
      "  normalize_poissonian_noise=True\n",
      "  algorithm=NMF\n",
      "  output_dimension=7\n",
      "  centre=None\n",
      "scikit-learn estimator:\n",
      "NMF(n_components=7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macark/hyperspy-bundle/lib/python3.8/site-packages/hyperspy/misc/eds/utils.py:627: RuntimeWarning: invalid value encountered in true_divide\n",
      "  composition = number_of_atoms / total_atoms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-250 quantified\n",
      "i=0...done.\n",
      "<Signal2D, title: HAADF, dimensions: (442|532, 928)>\n",
      "Decomposition info:\n",
      "  normalize_poissonian_noise=True\n",
      "  algorithm=SVD\n",
      "  output_dimension=None\n",
      "  centre=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macark/hyperspy-bundle/lib/python3.8/site-packages/sklearn/decomposition/_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn((\"The 'init' value, when 'init=None' and \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposition info:\n",
      "  normalize_poissonian_noise=True\n",
      "  algorithm=NMF\n",
      "  output_dimension=7\n",
      "  centre=None\n",
      "scikit-learn estimator:\n",
      "NMF(n_components=7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macark/hyperspy-bundle/lib/python3.8/site-packages/hyperspy/misc/eds/utils.py:627: RuntimeWarning: invalid value encountered in true_divide\n",
      "  composition = number_of_atoms / total_atoms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100-350 quantified\n",
      "i=1...done.\n",
      "<Signal2D, title: HAADF, dimensions: (442|532, 928)>\n",
      "Decomposition info:\n",
      "  normalize_poissonian_noise=True\n",
      "  algorithm=SVD\n",
      "  output_dimension=None\n",
      "  centre=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macark/hyperspy-bundle/lib/python3.8/site-packages/sklearn/decomposition/_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn((\"The 'init' value, when 'init=None' and \"\n",
      "/Users/macark/hyperspy-bundle/lib/python3.8/site-packages/sklearn/decomposition/_nmf.py:1090: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\"Maximum number of iterations %d reached. Increase it to\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decomposition info:\n",
      "  normalize_poissonian_noise=True\n",
      "  algorithm=NMF\n",
      "  output_dimension=7\n",
      "  centre=None\n",
      "scikit-learn estimator:\n",
      "NMF(n_components=7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macark/hyperspy-bundle/lib/python3.8/site-packages/hyperspy/misc/eds/utils.py:627: RuntimeWarning: invalid value encountered in true_divide\n",
      "  composition = number_of_atoms / total_atoms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191-441 quantified\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "height = int(np.ceil(no_of_frames_1/(window_range-overlap)))\n",
    "shape = (7, 116, 66) #Again this can be specifically determined but I find it easier to just set it manually.\n",
    "#First number is the number of frames you will get after processing and then x-dimension, y-dimension).\n",
    "Pt_counts = np.zeros(shape)\n",
    "Ni_counts = np.zeros(shape)\n",
    "\n",
    "quantify = True\n",
    "threshold = 10 #needed for making composition maps of nanoparticles.\n",
    "\n",
    "if quantify == True:\n",
    "    Ni_atoms = np.zeros(shape)\n",
    "    Pt_atoms = np.zeros(shape)\n",
    "    \n",
    "    Ni_composition = np.zeros(shape)\n",
    "    Pt_composition = np.zeros(shape)\n",
    "    \n",
    "i=0\n",
    "\n",
    "while (i * overlap + window_range) < no_of_frames_1:\n",
    "\n",
    "    sc, maps = open_and_save_maps(save_folder+'/',\n",
    "                              file_location + file_name_1, \n",
    "                              start_window=i*overlap, \n",
    "                              end_window=((i*overlap)+window_range),\n",
    "                              ADF_location=adf_location,\n",
    "                              beam_current=beam_current,\n",
    "                              elements=elements,\n",
    "                              lines=lines,\n",
    "                              save=True,\n",
    "                              no_of_components = no_of_components,\n",
    "                              bin_scale = bin_scale,\n",
    "                              iw=None, \n",
    "                              bw=None)\n",
    "\n",
    "    \n",
    "    Pt_counts[i] = (maps[1] + maps[2]).data ##This part here needs to be modified to include all the elements you want to analyse.\n",
    "    Ni_counts[i] = (maps[0]).data\n",
    "    \n",
    "    if quantify == True:\n",
    "        results = quantify_and_save(save_folder+'/', \n",
    "                      sc, maps, cs, \n",
    "                      start_window=i*overlap, \n",
    "                      end_window=((i*overlap)+window_range),\n",
    "                      threshold=10,\n",
    "                      save=False)\n",
    "    \n",
    "        Ni_composition[int(i/(window_range-overlap))] = results[0].data\n",
    "        Pt_composition[int(i/(window_range-overlap))] = results[1].data\n",
    "        Ni_atoms[int(i/(window_range-overlap))] = results[2].data\n",
    "        Pt_atoms[int(i/(window_range-overlap))] = results[3].data\n",
    "    \n",
    "    print ('i='+str(i)+'...done.')\n",
    "    \n",
    "    i += 1\n",
    "    \n",
    "sc, maps = open_and_save_maps (save_folder+'/',\n",
    "                           file_location + file_name_1, \n",
    "                           start_window=(no_of_frames_1-1-window_range), \n",
    "                           end_window=(no_of_frames_1-1),\n",
    "                           ADF_location=adf_location,\n",
    "                           beam_current=beam_current,\n",
    "                           elements=elements,\n",
    "                           lines=lines,\n",
    "                           save=True,\n",
    "                           no_of_components = no_of_components,\n",
    "                           bin_scale = bin_scale,\n",
    "                           iw=None,\n",
    "                           bw=None)\n",
    "\n",
    "if quantify == True:\n",
    "    results = quantify_and_save(save_folder+'/', \n",
    "                  sc, maps, cs, \n",
    "                  start_window=(no_of_frames_1-1-window_range), \n",
    "                  end_window=(no_of_frames_1-1),\n",
    "                  threshold=10,\n",
    "                  save=False)\n",
    "\n",
    "    Ni_composition[int(i/(window_range-overlap))] = results[0].data\n",
    "    Pt_composition[int(i/(window_range-overlap))] = results[1].data\n",
    "    Ni_atoms[int(i/(window_range-overlap))] = results[2].data\n",
    "    Pt_atoms[int(i/(window_range-overlap))] = results[3].data\n",
    "\n",
    "Pt_counts[-1] = (maps[1] + maps[2]).data\n",
    "Ni_counts[-1] = (maps[0]).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwrite '/Users/macark/Desktop/2020.09.02_ChemiSTEM_LATPheating/1247/1247_Al_counts.hspy' (y/n)?\n",
      "y\n",
      "Overwrite '/Users/macark/Desktop/2020.09.02_ChemiSTEM_LATPheating/1247/1247_Al_counts.tif' (y/n)?\n",
      "y\n",
      "Overwrite '/Users/macark/Desktop/2020.09.02_ChemiSTEM_LATPheating/1247/1247_C_counts.hspy' (y/n)?\n",
      "y\n",
      "Overwrite '/Users/macark/Desktop/2020.09.02_ChemiSTEM_LATPheating/1247/1247_C_counts.tif' (y/n)?\n",
      "y\n",
      "Overwrite '/Users/macark/Desktop/2020.09.02_ChemiSTEM_LATPheating/1247/1247_O_counts.hspy' (y/n)?\n",
      "y\n",
      "Overwrite '/Users/macark/Desktop/2020.09.02_ChemiSTEM_LATPheating/1247/1247_O_counts.tif' (y/n)?\n",
      "y\n",
      "Overwrite '/Users/macark/Desktop/2020.09.02_ChemiSTEM_LATPheating/1247/1247_P_counts.hspy' (y/n)?\n",
      "y\n",
      "Overwrite '/Users/macark/Desktop/2020.09.02_ChemiSTEM_LATPheating/1247/1247_P_counts.tif' (y/n)?\n",
      "y\n",
      "Overwrite '/Users/macark/Desktop/2020.09.02_ChemiSTEM_LATPheating/1247/1247_Ti_counts.hspy' (y/n)?\n",
      "y\n",
      "Overwrite '/Users/macark/Desktop/2020.09.02_ChemiSTEM_LATPheating/1247/1247_Ti_counts.tif' (y/n)?\n",
      "y\n"
     ]
    }
   ],
   "source": [
    "Pt_counts = hs.signals.Signal2D(Pt_counts)\n",
    "Pt_counts.change_dtype('float32')\n",
    "Ni_counts = hs.signals.Signal2D(Ni_counts)\n",
    "Ni_counts.change_dtype('float32')\n",
    " \n",
    "Pt_counts.save(save_folder+'/1534_Pt_counts.hspy')\n",
    "Pt_counts.save(save_folder+'/1534_Pt_counts.tif')\n",
    "Ni_counts.save(save_folder+'/1534_Ni_counts.hspy')\n",
    "Ni_counts.save(save_folder+'/1534_Ni_counts.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_and_save_maps(file_location, \n",
    "                       file_name, \n",
    "                       start_window, \n",
    "                       end_window, \n",
    "                       ADF_location,\n",
    "                       beam_current,\n",
    "                       elements,\n",
    "                       lines,\n",
    "                       save,\n",
    "                       no_of_components,\n",
    "                       bin_scale,\n",
    "                       iw=None, \n",
    "                       bw=None):\n",
    "\n",
    "    \"\"\"\n",
    "    This function imports and data set and then saves the ADF, and \n",
    "    intensity maps. Whilst returning intensity maps for further use.\n",
    "    \"\"\"\n",
    "    #Import data set in given window range.\n",
    "    s = hs.load(file_name, first_frame=start_window, last_frame=end_window, load_SI_image_stack=True)\n",
    "    if save==True:\n",
    "        #Save ADF image.\n",
    "        short_name = str(start_window)+'-'+str(end_window)\n",
    "        adf = s[ADF_location].inav[start_window:end_window]\n",
    "        adf = adf.rebin(scale=(end_window-start_window,1, 1)).squeeze()\n",
    "        Image.fromarray(adf.data).save(file_location + '/'+ short_name +'_adf.tiff')\n",
    "    \n",
    "    #Extract and rebin EDX data cube.\n",
    "    s = s[-1]\n",
    "    s.set_microscope_parameters(beam_current = beam_current, live_time = float(s.original_metadata.Scan.DwellTime)*float(end_window-start_window))\n",
    "    s= s.rebin(scale=bin_scale)\n",
    "    s = s.isig[0.1:]\n",
    "\n",
    "    \n",
    "    #Decompose EDX\n",
    "    s.decomposition()\n",
    "    s.plot_explained_variance_ratio()\n",
    "    s.decomposition(algorithm='NMF', output_dimension=7)\n",
    "    #s.plot_decomposition_loadings(no_of_components)\n",
    "    sc = s.get_decomposition_model(no_of_components)\n",
    "    sc.add_elements(elements)\n",
    "    sc.set_lines(lines)\n",
    "    \n",
    "    #Set up for intensity extraction\n",
    "    if bw==None:\n",
    "        bw = sc.estimate_background_windows(line_width=[3., 3.])\n",
    "    if iw==None:\n",
    "        iw = sc.estimate_integration_windows(windows_width=1)\n",
    "    \n",
    "    maps = sc.get_lines_intensity()\n",
    "\n",
    "    return sc, maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantify_and_save(EDXSeries, \n",
    "                      s, \n",
    "                      maps, \n",
    "                      cs, \n",
    "                      start_window, \n",
    "                      end_window, \n",
    "                      threshold, \n",
    "                      save=True):\n",
    "    \"\"\"\n",
    "    This function imports and data set and then saves the ADF, and \n",
    "    intensity maps. Whilst returning intensity maps for further use.\n",
    "    \"\"\"\n",
    "    \n",
    "    #Quantify\n",
    "    results = s.quantification(maps, \n",
    "                               method='cross_section',\n",
    "                               factors = cs,\n",
    "                               absorption_correction=False)\n",
    "    \n",
    "    #Create background cancelled maps.\n",
    "    #results[1][2].plot()\n",
    "    binary = (((results[1][0] + results[1][1] + results[1][2] + results[1][3]))>threshold)\n",
    "    #binary.plot()\n",
    "    Ni_comp = (results[0][0] + results[0][1]) * binary \n",
    "    Pt_comp = (results[0][2] + results[0][3]) * binary \n",
    "    Ni_counts = (results[1][0] + results[1][1]) * binary \n",
    "    Pt_counts = (results[1][2] + results[1][3]) * binary \n",
    "    short_name = str(start_window)+'-'+str(end_window)\n",
    "    print(short_name, 'quantified')\n",
    "    \n",
    "    if save == True:\n",
    "        hs.plot.plot_images([Ni_comp, Pt_comp], scalebar='all', cmap='viridis', #colorbar='single',\n",
    "                   centre_colormap='False', tight_layout=True,\n",
    "                   label=['Ni','Pt'],\n",
    "                   axes_decor='off',vmin=0)\n",
    "        \n",
    "        #matplotlib.pyplot.savefig(EDXSeries + '/' + short_name + '_compound2.tif')\n",
    "        \n",
    "        Image.fromarray(Ni_counts.data).save(EDXSeries + '/' + short_name+'_Ni_atoms.tiff')\n",
    "        Image.fromarray(Pt_counts.data).save(EDXSeries + '/' + short_name+'_Pt_atoms.tiff')\n",
    "        Image.fromarray(Ni_comp.data).save(EDXSeries + '/' + short_name+'_Ni_composition.tiff')\n",
    "        Image.fromarray(Pt_comp.data).save(EDXSeries + '/' + short_name+'_Pt_composition.tiff')\n",
    "\n",
    "    return (Ni_comp, Pt_comp, Ni_counts, Pt_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
